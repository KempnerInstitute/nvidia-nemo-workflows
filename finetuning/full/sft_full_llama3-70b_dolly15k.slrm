#!/bin/bash
#SBATCH --job-name="n2-sft-test"     # a name for your job
#SBATCH --partition=<partition>   # partition to which job should be submitted
#SBATCH --account=<account> 
#SBATCH --nodes=8                                 # node count
#SBATCH --ntasks-per-node=4                                # total number of tasks across all nodes
#SBATCH --time=01:30:00                           # total run time limit (HH:MM:SS)
#SBATCH --gpus-per-node=4                      # Number of GPUs per node (4 per node)
#SBATCH --cpus-per-task=16
#SBATCH --mem=0
#SBATCH --output %x_%N_%j.out           # Output file
#SBATCH --error %x_%N_%j.out             # Error file

export GPUS_NODE=$SLURM_NTASKS_PER_NODE
export NNODES=$SLURM_NNODES
export SCHEME="none"
CONCAT_SAMPLING_PROBS="[1]"
export TP_SIZE=4
export PP_SIZE=4

export NCCL_DEBUG=WARN
nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

for head_port in {20000..40000}; do ! nc -z localhost ${myport} && break; done
echo $head_port


export MASTER_ADDR=$head_node_ip
export MASTER_PORT=$head_port
world_size=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
export WORLD_SIZE=$world_size

echo $MASTER_ADDR $MASTER_PORT
echo "WORLD_SIZE=$world_size"


data_pth="/n/holylfs06/LABS/kempner_shared/Everyone/workflow/sft_llm/nemo_data/databricks-dolly-15k"
model_pth="/n/holylfs06/LABS/kempner_shared/Everyone/containers/mlperf_benchmarking/nemo_files/model/"
MODEL="$model_pth/llama3-70b.nemo"
TRAIN_DS="[$data_pth/training.jsonl]"
VALID_DS="[$data_pth/validation.jsonl]"
TEST_DS="[$data_pth/test.jsonl]"
VALID_NAMES="databricks-dolly"
OUTPUT_DIR="./sft-output-test"


#CONCAT_SAMPLING_PROBS="[0.3,0.7]"

echo "NNODES GPUS_NODE WORLD_SIZE MASTER_ADDR MASTER_PORT"
echo "$NNODES $GPUS_NODE $WORLD_SIZE $MASTER_ADDR $MASTER_PORT"

echo $CMD
read -r -d '' cmd <<EOF
   python3 /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \
   trainer.devices=${GPUS_NODE} \
   trainer.num_nodes=${NNODES} \
   model.micro_batch_size=1 \
   model.global_batch_size=8 \
   trainer.precision=bf16-mixed \
   trainer.val_check_interval=0.1 \
   trainer.max_steps=50 \
   model.restore_from_path=${MODEL} \
   model.peft.peft_scheme=${SCHEME} \
   model.data.train_ds.file_names=${TRAIN_DS} \
   model.data.validation_ds.file_names=${VALID_DS} \
   model.tensor_model_parallel_size=${TP_SIZE} \
   model.pipeline_model_parallel_size=${PP_SIZE} \
   model.data.train_ds.concat_sampling_probabilities=[1.0] \
   model.megatron_amp_O2=True \
   ++model.mcore_gpt=True \
   ++model.dist_ckpt_load_strictness=log_all \
   exp_manager.exp_dir=${OUTPUT_DIR} \
   exp_manager.explicit_log_dir=${OUTPUT_DIR} 

EOF

echo $cmd

env 
echo "=================== Launching JoB ================="

container_file="/n/holylfs06/LABS/kempner_shared/Everyone/containers/mlperf_benchmarking/nemo-25.02.rc0.simg"
srun singularity run --nv $container_file $cmd 

echo "=================== Finished JoB ================="

