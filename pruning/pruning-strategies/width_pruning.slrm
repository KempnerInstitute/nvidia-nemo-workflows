#!/bin/bash
#SBATCH --job-name="width-pruning"              # a name for your job
#SBATCH --partition=                            # <---- partition to which job should be submitted
#SBATCH --account=                              # <---- account to which job should be charged
#SBATCH --nodes=1                               # node count
#SBATCH --ntasks-per-node=4                     # total number of tasks across all nodes
#SBATCH --time=01:00:00                         # total run time limit (HH:MM:SS)
#SBATCH --gpus-per-node=4                       # Number of GPUs per node (4 per node)
#SBATCH --cpus-per-task=10
#SBATCH --mem=320G
#SBATCH --output %x_%N_%j.out                   # Output file
#SBATCH --error %x_%N_%j.out                    # Error file

export GPUS_NODE=$SLURM_GPUS_PER_NODE
export NNODES=$SLURM_NNODES
export TP_SIZE=1
export PP_SIZE=4

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

for head_port in {20000..30000}; do ! nc -z localhost ${head_port} && break; done
echo $head_port

export MASTER_ADDR=$head_node_ip
export MASTER_PORT=$head_port
world_size=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
export WORLD_SIZE=$world_size

echo $MASTER_ADDR $MASTER_PORT
echo "WORLD_SIZE=$world_size"


nemo2_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/models/llama-3_1-8b-nemo2.nemo
output_model_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/models/pruned-llama-3_1-8b-width-pruning
output_index_mapping_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/models/index-mapping/pruned-llama-3_1-8b-width-pruning
data_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/data/wikitext_tokenized_train_text_document
seq_length=8192
target_ffn_hidden_size=9216
target_hidden_size=3072
target_num_attention_heads=32
target_num_query_groups=8

echo $CMD
read -r -d '' cmd <<EOF
torchrun --nproc_per_node $GPUS_NODE /opt/NeMo/scripts/llm/gpt_prune.py \
    --devices $GPUS_NODE \
    --tp_size $TP_SIZE \
    --pp_size $PP_SIZE \
    --restore_path $nemo2_path \
    --seq_length $seq_length \
    --data_paths 1.0 $data_path \
    --index_mapping_dir $output_index_mapping_path \
    --target_ffn_hidden_size $target_ffn_hidden_size \
    --target_hidden_size $target_hidden_size \
    --target_num_attention_heads $target_num_attention_heads \
    --target_num_query_groups $target_num_query_groups \
    --save_path $output_model_path
EOF

echo $cmd

env 
echo "=================== Launching JoB ================="

container_file="/n/holylfs06/LABS/kempner_shared/Everyone/containers/mlperf_benchmarking/nemo_25.04.sif"
srun singularity run --nv $container_file $cmd 

echo "=================== Finished JoB =================="
