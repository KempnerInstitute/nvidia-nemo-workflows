#!/bin/bash
#SBATCH --job-name="data-preparation"           # a name for your job
#SBATCH --partition=                            # <---- partition to which job should be submitted
#SBATCH --account=                              # <---- account to which job should be charged
#SBATCH --nodes=1                               # node count
#SBATCH --ntasks-per-node=4                     # total number of tasks across all nodes
#SBATCH --time=01:00:00                         # total run time limit (HH:MM:SS)
#SBATCH --gpus-per-node=4                       # Number of GPUs per node (4 per node)
#SBATCH --cpus-per-task=10
#SBATCH --mem=320G
#SBATCH --output %x_%N_%j.out                   # Output file
#SBATCH --error %x_%N_%j.out                    # Error file

export GPUS_NODE=$SLURM_GPUS_PER_NODE
export NNODES=$SLURM_NNODES
export TP_SIZE=1
export PP_SIZE=4

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

for head_port in {20000..30000}; do ! nc -z localhost ${head_port} && break; done
echo $head_port

export MASTER_ADDR=$head_node_ip
export MASTER_PORT=$head_port
world_size=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
export WORLD_SIZE=$world_size

echo $MASTER_ADDR $MASTER_PORT
echo "WORLD_SIZE=$world_size"


nemo2_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/models/llama-3_1-8b-nemo2.nemo
output_path=/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/models/pruned-llama-3_1-8b-drop-layers
layers_to_drop="16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31"


echo $CMD
read -r -d '' cmd <<EOF
torchrun --nproc_per_node $GPUS_NODE /opt/NeMo/scripts/llm/gpt_prune.py \
    --devices $GPUS_NODE \
    --tp_size $TP_SIZE \
    --pp_size $PP_SIZE \
    --restore_path $nemo2_path \
    --drop_layers $layers_to_drop \
    --save_path $output_path
EOF

echo $cmd

env 
echo "=================== Launching JoB ================="

container_file="/n/holylfs06/LABS/kempner_shared/Everyone/containers/mlperf_benchmarking/nemo_25.04.sif"
srun singularity run --nv $container_file $cmd 

echo "=================== Finished JoB =================="
