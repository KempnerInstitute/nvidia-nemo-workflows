#!/bin/bash
#SBATCH --job-name="logit-distillation"         # a name for your job
#SBATCH --partition=                            # <---- partition to which job should be submitted
#SBATCH --account=                              # <---- account to which job should be charged
#SBATCH --nodes=1                               # node count
#SBATCH --ntasks-per-node=4                     # total number of tasks across all nodes
#SBATCH --time=01:00:00                         # total run time limit (HH:MM:SS)
#SBATCH --gpus-per-node=4                       # Number of GPUs per node (4 per node)
#SBATCH --cpus-per-task=10
#SBATCH --mem=320G
#SBATCH --output %x_%N_%j.out                   # Output file
#SBATCH --error %x_%N_%j.out                    # Error file

export GPUS_NODE=$SLURM_GPUS_PER_NODE
export NNODES=$SLURM_NNODES
export TP_SIZE=4
export PP_SIZE=1
export CP_SIZE=1
export DP_SIZE=1

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

for head_port in {20000..30000}; do ! nc -z localhost ${head_port} && break; done
echo $head_port

export MASTER_ADDR=$head_node_ip
export MASTER_PORT=$head_port
world_size=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))
export WORLD_SIZE=$world_size

echo $MASTER_ADDR $MASTER_PORT
echo "WORLD_SIZE=$world_size"


student_nemo_path="/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows-clean/nvidia-nemo-workflows/models/llama3.1-8b_nemo2"
teacher_nemo_path="/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows-clean/nvidia-nemo-workflows/models/llama3.1-8b_nemo2"
data_path="/n/holylfs06/LABS/kempner_undergrads/Lab/acherilyn/nvidia-nemo-workflows/data/wikitext_tokenized_test_text_document"
tokenizer_library=huggingface
model_id=meta-llama/Llama-3.1-8B

name="distill_testrun"
log_dir="./distill_logs/"

seq_length=8192
steps=100

export MICRO_BATCHSIZE=1
export GLOBAL_BATCHSIZE=4
export NUM_NODES=1
export DEVICES_PER_NODE=4
export NPROC_PER_NODE=$(($TP_SIZE * $CP_SIZE * $PP_SIZE * $DP_SIZE))

echo $CMD
read -r -d '' cmd <<EOF
torchrun --nproc_per_node=$NPROC_PER_NODE /opt/NeMo/scripts/llm/gpt_distillation.py \
    --name $name \
    --student_path $student_nemo_path \
    --teacher_path $teacher_nemo_path \
    --tokenizer_library $tokenizer_library \
    --tokenizer_type $model_id \
    --tp_size $TP_SIZE \
    --cp_size $CP_SIZE \
    --pp_size $PP_SIZE \
    --devices $DEVICES_PER_NODE \
    --num_nodes $NUM_NODES \
    --log_dir $log_dir \
    --max_steps $steps \
    --gbs $GLOBAL_BATCHSIZE \
    --mbs $MICRO_BATCHSIZE \
    --data_paths 1.0 $data_path \
    --seq_length $seq_length
EOF

echo $cmd

env 
echo "=================== Launching JoB ================="

container_file="/n/holylfs06/LABS/kempner_shared/Everyone/containers/mlperf_benchmarking/nemo_25.04.sif"
srun singularity run --nv \
    --bind distillation/modified-container-files/gpt_distillation.py:/opt/NeMo/scripts/llm/gpt_distillation.py \
    $container_file $cmd 

echo "=================== Finished JoB =================="
